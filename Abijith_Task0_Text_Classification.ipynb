{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn import metrics\n",
    "\n"
   ],
   "metadata": {
    "id": "7rS4RFdcaMIp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8497eb48-5970-4a4f-ce23-adea7e1a5aaa",
    "ExecuteTime": {
     "end_time": "2023-11-13T16:50:21.471232100Z",
     "start_time": "2023-11-13T16:50:16.622059100Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]"
   ],
   "metadata": {
    "id": "lllNTARdKO4p",
    "ExecuteTime": {
     "end_time": "2023-11-13T16:50:21.475542100Z",
     "start_time": "2023-11-13T16:50:21.474859200Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "DUc4cKDRqqYt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(f\"{len(data.filenames)} documents\")\n",
    "print(f\"{len(data.target_names)} categories\")\n",
    "print()"
   ],
   "metadata": {
    "id": "88Af_mD8cpiX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "61951a93-3cef-497b-e71f-ea530f64fd5d",
    "ExecuteTime": {
     "end_time": "2023-11-13T16:50:21.699884900Z",
     "start_time": "2023-11-13T16:50:21.475542100Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857 documents\n",
      "2 categories\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=42)"
   ],
   "metadata": {
    "id": "qGYgHq2ZcwwN",
    "ExecuteTime": {
     "end_time": "2023-11-13T16:50:21.705290Z",
     "start_time": "2023-11-13T16:50:21.699884900Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class Word2VecWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=100, window=5, min_count=1, workers=4):\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model = Word2Vec(X, vector_size=self.size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Return the average word vector for each document\n",
    "        return np.array([\n",
    "            np.mean([self.model.wv[word] for word in words if word in self.model.wv]\n",
    "                    or [np.zeros(self.size)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"size\": self.size, \"window\": self.window, \"min_count\": self.min_count, \"workers\": self.workers}\n",
    "\n",
    "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        tagged_data = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(X)]\n",
    "        self.model = Doc2Vec(tagged_data, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.model.infer_vector(doc.split()) for doc in X])\n"
   ],
   "metadata": {
    "id": "U03_MQNNXJaA",
    "ExecuteTime": {
     "end_time": "2023-11-13T16:50:21.766806600Z",
     "start_time": "2023-11-13T16:50:21.757452900Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text.lower())\n",
    "feature_extractors = [\n",
    "    ('CountVectorizer', CountVectorizer()),\n",
    "    # ('Word2Vec',  Word2VecWrapper()),\n",
    "    # ('Doc2Vec', Doc2Vec(vector_size=50, min_count=2, epochs=40))\n",
    "]\n",
    "classifiers = [\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Support Vector Machine', SVC()),\n",
    "    ('Decision Tree', DecisionTreeClassifier())\n",
    "]\n",
    "results_table = pd.DataFrame(columns=['Feature Extractor', 'Classifier', 'Accuracy', 'Best Params'])"
   ],
   "metadata": {
    "id": "3RVKfYFIY3Ca",
    "ExecuteTime": {
     "end_time": "2023-11-13T16:50:21.777883700Z",
     "start_time": "2023-11-13T16:50:21.766806600Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define parameter grids for each pipeline\n",
    "param_grids = {\n",
    "    'Multinomial Naive Bayes': [{\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'clf__alpha': [1.0, 0.1, 0.01]\n",
    "    }],\n",
    "    'Logistic Regression': [{\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    }],\n",
    "    'Support Vector Machine': [{\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    }],\n",
    "    'Decision Tree': [{\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'clf__max_depth': [10, 20, None],\n",
    "        'clf__min_samples_split': [2, 5, 10]\n",
    "    }]\n",
    "}"
   ],
   "metadata": {
    "id": "BKTG1FbtJH_t",
    "ExecuteTime": {
     "end_time": "2023-11-13T16:50:21.784133900Z",
     "start_time": "2023-11-13T16:50:21.777883700Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for extractor_name, extractor_model in feature_extractors:\n",
    "    for clf_name, clf_model in classifiers:\n",
    "        pipelines = Pipeline([\n",
    "            ('vect', extractor_model),\n",
    "            ('clf', clf_model),\n",
    "        ])\n",
    "        grid_search = GridSearchCV(pipelines, param_grids[clf_name], cv=5, n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(data.data, data.target)\n",
    "        print(f\"Best score for {clf_name}: {grid_search.best_score_:.3f}\")\n",
    "        print(\"Best parameters set:\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # Iterate over the parameter names specific to the current model's grid\n",
    "        # for param_name in param_grids[clf_name]:\n",
    "        #     # Corrected access to the best parameters\n",
    "        #     print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjOx5Yyjakcb",
    "outputId": "626c5a70-9905-46c5-e797-cce289c46872",
    "ExecuteTime": {
     "end_time": "2023-11-13T16:50:57.733624700Z",
     "start_time": "2023-11-13T16:50:21.784133900Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best score for Multinomial Naive Bayes: 0.946\n",
      "Best parameters set:\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for Logistic Regression: 0.945\n",
      "Best parameters set:\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best score for Support Vector Machine: 0.938\n",
      "Best parameters set:\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for Decision Tree: 0.891\n",
      "Best parameters set:\n"
     ]
    }
   ]
  }
 ]
}
